{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c932cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/gap/gap-development.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca37e0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "              ID                                               Text Pronoun  \\\n0  development-1  Zoe Telford -- played the police officer girlf...     her   \n1  development-2  He grew up in Evanston, Illinois the second ol...     His   \n2  development-3  He had been reelected to Congress, but resigne...     his   \n3  development-4  The current members of Crime have also perform...     his   \n4  development-5  Her Santa Fe Opera debut in 2005 was as Nuria ...     She   \n\n   Pronoun-offset                  A  A-offset  A-coref                B  \\\n0             274     Cheryl Cassidy       191     True          Pauline   \n1             284          MacKenzie       228     True    Bernard Leach   \n2             265            Angeloz       173    False       De la Sota   \n3             321               Hell       174    False  Henry Rosenthal   \n4             437  Kitty Oppenheimer       219    False           Rivera   \n\n   B-offset  B-coref                                                URL  \n0       207    False  http://en.wikipedia.org/wiki/List_of_Teachers_...  \n1       251    False      http://en.wikipedia.org/wiki/Warren_MacKenzie  \n2       246     True  http://en.wikipedia.org/wiki/Jos%C3%A9_Manuel_...  \n3       336     True          http://en.wikipedia.org/wiki/Crime_(band)  \n4       294     True        http://en.wikipedia.org/wiki/Jessica_Rivera  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Text</th>\n      <th>Pronoun</th>\n      <th>Pronoun-offset</th>\n      <th>A</th>\n      <th>A-offset</th>\n      <th>A-coref</th>\n      <th>B</th>\n      <th>B-offset</th>\n      <th>B-coref</th>\n      <th>URL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>development-1</td>\n      <td>Zoe Telford -- played the police officer girlf...</td>\n      <td>her</td>\n      <td>274</td>\n      <td>Cheryl Cassidy</td>\n      <td>191</td>\n      <td>True</td>\n      <td>Pauline</td>\n      <td>207</td>\n      <td>False</td>\n      <td>http://en.wikipedia.org/wiki/List_of_Teachers_...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>development-2</td>\n      <td>He grew up in Evanston, Illinois the second ol...</td>\n      <td>His</td>\n      <td>284</td>\n      <td>MacKenzie</td>\n      <td>228</td>\n      <td>True</td>\n      <td>Bernard Leach</td>\n      <td>251</td>\n      <td>False</td>\n      <td>http://en.wikipedia.org/wiki/Warren_MacKenzie</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>development-3</td>\n      <td>He had been reelected to Congress, but resigne...</td>\n      <td>his</td>\n      <td>265</td>\n      <td>Angeloz</td>\n      <td>173</td>\n      <td>False</td>\n      <td>De la Sota</td>\n      <td>246</td>\n      <td>True</td>\n      <td>http://en.wikipedia.org/wiki/Jos%C3%A9_Manuel_...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>development-4</td>\n      <td>The current members of Crime have also perform...</td>\n      <td>his</td>\n      <td>321</td>\n      <td>Hell</td>\n      <td>174</td>\n      <td>False</td>\n      <td>Henry Rosenthal</td>\n      <td>336</td>\n      <td>True</td>\n      <td>http://en.wikipedia.org/wiki/Crime_(band)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>development-5</td>\n      <td>Her Santa Fe Opera debut in 2005 was as Nuria ...</td>\n      <td>She</td>\n      <td>437</td>\n      <td>Kitty Oppenheimer</td>\n      <td>219</td>\n      <td>False</td>\n      <td>Rivera</td>\n      <td>294</td>\n      <td>True</td>\n      <td>http://en.wikipedia.org/wiki/Jessica_Rivera</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f33c066d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 11929, 11341,  2821,   118,   118,  1307,  1103,  2021,  2575,\n",
      "          6124,  1104,  3274,   117,  8153,   119, 12786,  8223,  1174,  1118,\n",
      "          3274,  1107,  1103,  1509,  2004,  1104,  1326,   122,   117,  1170,\n",
      "          1119,  7362,  1114,  8067,   117,  1105,  1110,  1136,  1562,  1254,\n",
      "           119, 19704,  1819,  1307, 21173, 14190,   117, 16473,   112,   188,\n",
      "          1910,  1105,  1145,   170,  1214,  1429, 11602,  1107,  3274,   112,\n",
      "           188,  1705,   119, 12786,  8223,  1174,  1123,  6508,  1378,  3274,\n",
      "           112,   188,  5566,  1170,  1119,  2010,   112,   189,  1138,  2673,\n",
      "          1114,  1123,  1133,  1224, 11326,  1142,  1108,  1496,  1106,  1140,\n",
      "          9256, 24121,  1116,  1228,  1123,  1910, 16473,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-cased\", output_hidden_states=True)\n",
    "\n",
    "embeds = bert_tokenizer.encode_plus(df['Text'][0], is_split_into_words=False, return_tensors=\"pt\")\n",
    "\n",
    "print(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cfda77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 99])\n",
      "['[CLS]', 'Zoe', 'Tel', '##ford', '-', '-', 'played', 'the', 'police', 'officer', 'girlfriend', 'of', 'Simon', ',', 'Maggie', '.', 'Du', '##mp', '##ed', 'by', 'Simon', 'in', 'the', 'final', 'episode', 'of', 'series', '1', ',', 'after', 'he', 'slept', 'with', 'Jenny', ',', 'and', 'is', 'not', 'seen', 'again', '.', 'Phoebe', 'Thomas', 'played', 'Cheryl', 'Cassidy', ',', 'Pauline', \"'\", 's', 'friend', 'and', 'also', 'a', 'year', '11', 'pupil', 'in', 'Simon', \"'\", 's', 'class', '.', 'Du', '##mp', '##ed', 'her', 'boyfriend', 'following', 'Simon', \"'\", 's', 'advice', 'after', 'he', 'wouldn', \"'\", 't', 'have', 'sex', 'with', 'her', 'but', 'later', 'realised', 'this', 'was', 'due', 'to', 'him', 'catching', 'crab', '##s', 'off', 'her', 'friend', 'Pauline', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "input_ids = embeds['input_ids']\n",
    "print(input_ids.shape)\n",
    "print(bert_tokenizer.convert_ids_to_tokens(input_ids.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3275cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([97], dtype=int64),)\n",
      "tensor([ 2.4865e-02,  8.7189e-02, -2.9874e-01,  2.7464e-01, -3.4620e-02,\n",
      "        -1.8526e-01, -4.0381e-01, -3.2950e-01, -3.1077e-01, -9.6205e-01,\n",
      "         1.8459e-01,  8.9893e-01,  1.0140e-01, -2.2178e-03, -1.2092e+00,\n",
      "         4.6842e-01,  7.2310e-01,  2.6946e-01,  6.0602e-01,  3.3600e-01,\n",
      "        -6.9873e-01,  4.4655e-01,  1.2239e+00, -1.9887e-01,  1.1207e+00,\n",
      "         7.1256e-02, -9.7466e-02,  3.7977e-01, -4.9891e-01, -4.6798e-02,\n",
      "         4.9862e-01,  4.8732e-01,  1.0820e-01, -1.8058e-01, -3.7724e-01,\n",
      "        -2.1718e-01,  3.7783e-02,  4.1661e-01, -1.0268e-01, -8.5980e-01,\n",
      "         2.2138e-01, -2.2971e-01,  6.2703e-02, -1.4382e-01,  1.4447e-01,\n",
      "        -1.2565e-01, -1.5106e+00,  2.0652e-01,  7.5550e-01, -2.5890e-01,\n",
      "        -6.2316e-01, -6.9404e-01, -1.0389e-01, -8.5084e-02, -7.4667e-02,\n",
      "        -6.8613e-01,  1.2332e-01,  2.0314e-02, -3.4862e-01, -2.1305e-01,\n",
      "         8.3751e-01,  1.4084e-01,  9.8267e-01, -1.2952e-01,  1.3081e-01,\n",
      "         2.9403e-01, -3.9110e-01,  3.7766e-01,  1.6451e-02, -2.4213e-01,\n",
      "         1.9555e-01,  3.2884e-01,  1.0135e+00, -2.4809e-01,  2.4977e-01,\n",
      "        -8.6391e-05,  6.1483e-01, -8.0658e-01,  4.2937e-01,  7.2094e-02,\n",
      "         8.2721e-01,  7.6061e-01,  6.4917e-01,  3.4436e-01,  2.9614e-01,\n",
      "        -1.9178e-01,  3.2274e-01,  7.0622e-01,  1.2443e+00,  1.1342e-01,\n",
      "        -3.1113e-01,  5.5907e-01, -6.8794e-01,  5.1029e-01,  8.6719e-02,\n",
      "        -4.0308e-01,  5.4663e-01,  2.0363e-01, -4.6500e-02, -4.8646e-01,\n",
      "         2.5717e-01, -4.1793e-01,  1.7843e-01, -2.5513e-01, -2.5534e-01,\n",
      "        -2.0441e-01, -6.8123e-01,  5.1647e-01, -3.2448e-01, -6.5749e-01,\n",
      "        -8.9957e-02, -1.9062e-01,  6.3870e-01, -5.0370e-01, -1.5716e-01,\n",
      "        -7.8369e-02, -5.2722e-01,  8.9351e-01,  4.8373e-02,  2.1306e-01,\n",
      "         2.7827e-01,  3.8505e-01, -4.6850e-01, -2.1002e-01,  4.7119e-02,\n",
      "         3.9613e-03,  5.9905e-01, -2.8702e-01,  4.9441e-01,  6.7524e-01,\n",
      "        -1.0365e+00, -3.6551e-01, -3.1516e-01,  1.1495e-01, -3.6010e-01,\n",
      "        -1.1512e-01,  1.3988e-01, -4.7340e-02, -4.1244e-01,  3.9890e-01,\n",
      "        -8.0742e-01,  1.3983e-01,  7.5917e-01, -2.5145e-01,  4.7704e-02,\n",
      "         3.5511e-01,  6.0392e-01, -5.9250e-02,  1.1887e+00, -4.0339e-01,\n",
      "         7.3380e-02, -5.6543e-01,  3.6614e-02,  1.1599e-01,  6.3660e-01,\n",
      "        -4.4934e-01,  6.6891e-01, -1.3555e+00,  6.0230e-01, -6.2393e-01,\n",
      "        -7.0110e-01, -6.9336e-01, -6.0886e-01,  7.3753e-01, -4.3423e-01,\n",
      "        -6.4981e-01,  1.8034e-01, -3.0906e-01, -4.0169e-01,  1.0225e+00,\n",
      "         1.1820e-01, -1.7617e-01, -2.6099e-01, -9.5697e-01,  1.8331e-01,\n",
      "        -1.4535e-01, -1.1223e+00,  9.4403e-01, -4.5821e-02, -8.0977e-01,\n",
      "         5.5772e-01, -4.6455e-01,  8.7169e-01, -1.8237e-01, -6.5522e-01,\n",
      "         8.6745e-01,  5.4303e-01, -7.4487e-01,  1.2490e+00,  1.2825e-01,\n",
      "         6.7199e-01,  2.7468e-01, -7.2367e-01, -4.8542e-01, -1.3367e+00,\n",
      "        -5.9244e-01, -2.4502e-01,  1.2834e-02,  4.9780e-01,  1.1548e+00,\n",
      "        -8.0562e-01, -1.2936e-01,  6.0484e-02, -8.1347e-01,  1.5791e-01,\n",
      "         2.8747e-01,  1.7540e-01,  9.1171e-01,  3.6417e-01,  2.6126e-01,\n",
      "        -9.6115e-01, -2.6068e-01, -1.4925e-01,  6.7764e-01,  8.9421e-01,\n",
      "        -4.7411e-02,  1.0140e+00, -1.0568e+00,  2.1293e-01, -2.5769e-01,\n",
      "        -5.5079e-01,  4.5294e-01,  2.4653e-01,  1.5245e-01,  3.4828e-01,\n",
      "        -7.7709e-03, -6.9392e-01, -5.0945e-01, -1.4867e-01, -2.2762e-01,\n",
      "         4.3631e-01,  7.1015e-01, -1.2155e+00, -2.7521e-01, -2.1163e-01,\n",
      "         1.0079e+00, -4.7867e-01,  2.4467e-01,  4.2942e-01, -7.4210e-01,\n",
      "         4.4226e-01,  3.9620e-01,  1.1974e+00, -8.1136e-02,  7.1182e-02,\n",
      "         5.6670e-01,  1.5700e-01,  6.7849e-01,  2.5110e-01,  1.4633e-01,\n",
      "         1.2288e-01,  1.1584e-01, -2.9925e-01,  5.3467e-01,  2.3814e-01,\n",
      "         3.5525e-02,  2.6746e-01,  7.6197e-02,  2.5876e-01,  9.1991e-01,\n",
      "         5.0222e-01,  9.0236e-01,  4.7339e-01,  3.1290e-01, -2.0236e-01,\n",
      "        -3.2783e-01,  2.9073e-01, -1.0354e-01,  8.8063e-03,  2.3104e-01,\n",
      "         8.8172e-01, -6.5501e-01,  9.7237e-01, -6.5675e-01, -5.7867e-01,\n",
      "        -4.2320e-01,  4.6860e-01, -3.1538e-01, -8.6956e-02, -6.7373e-01,\n",
      "         6.7752e-01, -5.8219e-01, -5.7111e-01,  3.5282e-01, -4.7736e-01,\n",
      "        -7.0338e-01,  8.4823e-03, -2.8651e-01, -1.0419e-01, -4.4489e-01,\n",
      "         8.7065e-01, -5.4074e-01, -3.1247e-02,  5.2014e-01,  5.6101e-02,\n",
      "        -2.4869e-01, -3.4201e-01, -2.5140e-01, -2.6868e-01,  5.6492e-02,\n",
      "         8.5362e-01, -3.5043e-02,  1.9869e-01, -3.5708e-02, -2.8313e+00,\n",
      "         1.1005e-01,  2.4074e-01, -2.5569e-01, -5.3349e-01, -1.0667e-01,\n",
      "         5.6767e-01,  8.4302e-01,  5.1529e-01,  1.4395e-01, -3.9809e-01,\n",
      "        -6.0338e-01, -1.2262e-02,  2.0131e-02,  2.4909e-01, -5.7630e-03,\n",
      "        -7.2374e-01,  1.5933e+00, -3.0779e-01,  7.2313e-02, -8.8816e-01,\n",
      "         1.5026e-01,  2.8423e-01, -2.6715e-01, -1.2070e+00,  1.4179e-01,\n",
      "         3.1107e-01, -1.4837e-01,  4.6270e-02,  1.1755e+00,  3.1691e-02,\n",
      "         7.4775e-01, -4.0290e-01,  5.3578e-01,  7.4850e-01, -1.8236e-01,\n",
      "        -1.9135e-01, -3.9874e-01, -5.2670e-01, -1.4164e-01, -1.7069e-01,\n",
      "        -6.1696e-01,  5.8934e-02, -1.1725e-01,  7.3129e-02, -3.0096e-01,\n",
      "        -3.1986e-01, -5.0529e-01, -4.4795e-01,  1.2064e-01,  5.1545e-01,\n",
      "        -7.7249e-01,  7.7551e-02,  3.6457e-01, -1.2999e-01, -2.3244e-01,\n",
      "         1.6125e-01, -9.7595e-01,  6.9122e-01, -7.3203e-01,  1.3842e-01,\n",
      "        -9.0766e-02, -1.1959e+00, -2.8861e-01,  1.2679e+00,  7.6038e-03,\n",
      "         3.4541e-01, -6.7391e-01, -4.4235e-02, -3.0024e-01,  1.3896e-01,\n",
      "         4.7472e-01,  4.7850e-01,  1.6659e-01,  1.6561e-01, -2.0329e-01,\n",
      "         5.5372e-01,  1.6511e-01, -1.1771e+00, -4.5318e-01, -5.3845e-01,\n",
      "         6.0014e-01,  3.2011e-01,  8.7464e-02,  6.6427e-01, -5.9941e-02,\n",
      "         9.9133e-01, -7.2687e-01,  8.4659e-01, -5.3856e-01, -6.3897e-01,\n",
      "         2.5217e-01, -7.6079e-02,  6.2562e-01, -6.6577e-01,  1.7736e-01,\n",
      "        -1.7528e-01,  1.0683e-01, -3.5700e-01,  1.1619e+00, -8.4147e-02,\n",
      "         1.4762e-01, -3.2447e-01,  1.9394e-01, -1.1936e-01, -4.9726e-01,\n",
      "         9.1201e-03,  1.7874e-01, -9.2123e-02,  1.1728e+00,  6.3491e-01,\n",
      "         9.6932e-02,  3.2047e-01, -2.2288e-01, -3.2462e-01, -7.8400e-02,\n",
      "         1.0292e-01, -7.3664e-01, -4.5505e-01,  2.3069e-01,  7.0758e-01,\n",
      "        -8.8935e-01,  1.3740e-01, -3.0210e-01,  6.7541e-01,  8.3141e-01,\n",
      "        -1.5501e-01,  2.0026e-01, -1.6841e-01,  3.1843e-01,  5.7426e-02,\n",
      "        -4.5937e-02,  4.7725e-01,  2.5261e-01,  7.3363e-01,  3.7138e-01,\n",
      "        -2.3931e-01, -1.6514e+00,  5.6850e-01, -4.9928e-01,  7.8856e-01,\n",
      "        -4.1922e-01, -2.2844e-01,  1.9674e-01,  8.1402e-02, -2.2715e-01,\n",
      "        -4.0268e-03,  5.4345e-01,  5.6981e-01,  5.2727e-01, -2.4657e-01,\n",
      "        -7.4602e-01,  2.5812e-01,  2.5723e-02, -1.3765e+00,  4.7937e-01,\n",
      "         1.1041e-01,  1.8979e-02,  6.6689e-01,  1.0662e+00,  3.1388e-01,\n",
      "        -3.5043e-02, -8.0682e-01,  6.8211e-01, -5.4061e-01, -5.6919e-01,\n",
      "         6.8300e-02,  1.9201e-01, -7.8907e-01,  3.4272e-01, -4.4309e-01,\n",
      "        -1.4206e-01, -1.6813e-01, -1.6983e+00, -1.1522e+00,  4.1608e-01,\n",
      "        -5.2983e-01, -1.0132e+00, -3.8513e-01,  3.4644e-01, -7.6271e-02,\n",
      "         1.4612e-01,  2.6581e-01,  4.7270e-01,  4.6729e-01,  6.9661e-01,\n",
      "         3.1508e-01,  6.3012e-01, -4.7594e-01, -3.2538e-01,  1.0017e+00,\n",
      "         3.1206e-01,  1.1060e+00,  8.6316e-01, -1.1341e-01,  2.2067e-01,\n",
      "        -1.2910e+00, -4.3018e-01, -2.5174e-01,  3.7227e-01, -2.2769e-01,\n",
      "         8.4269e-01, -8.6520e-01,  2.0894e-01, -3.1337e-01,  9.6346e-01,\n",
      "        -9.9609e-01,  2.2384e-02, -1.9958e-01, -2.6908e-01, -7.2713e-01,\n",
      "         4.0112e-01,  1.0774e+00, -3.1569e-01, -1.9560e-01, -4.0359e-01,\n",
      "        -1.0606e-01,  1.7547e-01,  6.5810e-01, -4.8040e-01,  5.4025e-01,\n",
      "         6.6170e-01,  2.6975e-01, -8.6009e-01,  8.1247e-01, -9.2396e-01,\n",
      "        -1.0979e+00,  1.4014e-01, -3.9196e-01,  2.9057e-01,  6.7224e-01,\n",
      "         2.2954e-01, -5.3543e-01,  7.7385e-02, -2.4840e-01, -9.3115e-01,\n",
      "         4.0769e-01,  6.1994e-01,  5.6363e-01, -2.6268e-01,  6.5316e-01,\n",
      "        -1.7120e-02, -1.1884e-01, -2.2314e-01, -2.6429e-01, -4.6826e-01,\n",
      "        -4.5457e-01,  7.9126e-01,  1.7137e-01,  5.3855e-01,  7.0308e-02,\n",
      "         1.3127e+00, -6.8019e-01, -1.6037e+01, -2.5285e-01,  1.3300e-01,\n",
      "        -2.6715e-01,  2.3752e-01,  1.9932e-01, -7.7855e-01, -2.3535e-01,\n",
      "         3.7641e-01, -7.9164e-02, -5.4086e-01,  3.7906e-01,  6.4131e-01,\n",
      "         4.0441e-03, -1.6815e-01,  4.5523e-02,  8.5131e-02, -8.3416e-01,\n",
      "        -8.1626e-02, -1.1214e-01, -1.3041e+00, -1.3119e-01,  7.5930e-01,\n",
      "        -1.7424e-01,  1.2492e+00, -2.6853e-02, -1.8035e-01,  5.2428e-01,\n",
      "        -3.6466e-01, -3.5048e-01, -4.0818e-01,  5.2265e-01, -3.9012e-01,\n",
      "         6.0588e-01, -4.2435e-01, -7.1050e-01,  6.3499e-02,  6.8293e-01,\n",
      "        -3.4816e-02, -3.5355e-03, -9.7684e-01,  3.2005e-01,  3.8051e-01,\n",
      "        -4.0087e-01, -1.5476e-01, -4.3959e-02,  2.6272e-01, -3.8174e-01,\n",
      "         3.2001e-01, -3.9245e-01,  2.9306e-01,  3.0656e-01, -1.1932e+00,\n",
      "        -5.1428e-01, -2.0940e-01,  5.0516e-02, -6.7374e-01,  5.0940e-01,\n",
      "         9.2174e-03, -2.2806e-01,  9.4688e-01, -4.0531e-01, -1.8403e-01,\n",
      "        -5.1697e-01, -1.0821e+00, -5.1154e-01, -5.5105e-01,  5.5942e-02,\n",
      "         7.0911e-01, -1.1010e-02, -4.7738e-01, -8.9989e-01, -2.4669e-02,\n",
      "         1.8381e-01,  4.0182e-01,  2.4807e-01, -1.1706e+00,  2.8155e-01,\n",
      "        -3.6081e-01,  7.9487e-01,  3.7008e-01,  1.2005e+00, -1.0240e+00,\n",
      "        -5.3680e-01,  1.0571e+00,  3.0231e-01, -7.1954e-02,  7.2644e-01,\n",
      "         2.0421e-01,  7.3358e-01,  1.4552e-01, -7.9206e-01, -3.0353e-01,\n",
      "         1.2623e-01,  1.7342e-01,  1.9781e-01, -1.5411e-01,  4.5218e-02,\n",
      "        -2.0931e-01, -4.2359e-01,  4.6084e-03, -5.2705e-01, -1.0484e+00,\n",
      "        -1.0895e-01, -7.2082e-01, -9.9354e-02,  1.8679e-01, -1.3383e-01,\n",
      "        -5.6871e-01,  1.2569e+00,  3.1119e-01,  5.6947e-02,  2.3631e-01,\n",
      "         3.7528e-01, -8.6733e-02, -7.7910e-01,  3.4310e-01,  7.6690e-01,\n",
      "        -3.5310e-04,  1.0419e-01, -4.9483e-01, -7.6349e-01,  7.1468e-01,\n",
      "         5.7492e-01, -2.7752e-01, -5.6453e-01,  4.5255e-01,  8.0513e-02,\n",
      "         6.7758e-02, -3.5522e-01,  7.8920e-01, -5.7161e-02, -5.7386e-01,\n",
      "        -3.7578e-02,  3.2119e-01,  4.7180e-02,  8.3946e-01, -1.3389e-01,\n",
      "         5.6483e-01,  2.1964e-01, -2.9025e-01,  9.1110e-02,  3.4156e-01,\n",
      "        -7.2026e-01,  2.2249e-01, -8.2402e-01,  6.7960e-01, -4.8204e-01,\n",
      "        -2.2217e-01,  4.8621e-01, -8.9037e-01, -9.6721e-01, -1.2151e-01,\n",
      "        -4.4392e-01, -5.1678e-01,  6.1572e-01, -2.5088e-01, -2.2916e-01,\n",
      "        -1.8904e-01,  6.2322e-01,  5.9959e-01, -7.6262e-01,  2.3397e-01,\n",
      "         1.1339e+00, -5.5288e-02,  8.5890e-01, -3.5175e-01,  3.2092e-01,\n",
      "         7.5459e-01, -3.0122e-01,  8.5013e-01,  3.2655e-02, -5.8308e-02,\n",
      "         1.2587e-01,  1.4040e-01, -5.7459e-01,  9.7210e-01,  4.7191e-01,\n",
      "        -9.7703e-02, -6.6539e-01, -6.7872e-01, -3.4641e-01, -6.5886e-01,\n",
      "         1.1011e-01,  4.7215e-01, -5.4967e-02, -1.8913e-01, -3.9126e-01,\n",
      "        -3.2107e-01, -1.9881e-01, -2.4481e-01,  3.0124e-01, -1.2242e+00,\n",
      "         8.5651e-01,  4.9482e-03,  1.0932e+00, -7.0627e-02,  6.8954e-01,\n",
      "        -2.1219e-01,  6.6275e-01, -4.8103e-02, -2.0298e-01, -4.9485e-01,\n",
      "         7.2412e-01,  1.9651e-01, -1.1095e+00,  7.8446e-01, -1.0127e-01,\n",
      "        -6.0595e-01, -8.7335e-01,  1.2112e-01])\n"
     ]
    }
   ],
   "source": [
    "def get_word_embedding(encoded, output, idx):\n",
    "    token_ids_word = np.where(np.array(encoded.word_ids()) == idx)\n",
    "    print(token_ids_word)\n",
    "    word_tokens_output = output[token_ids_word]\n",
    "\n",
    "    return word_tokens_output.mean(dim=0)\n",
    "\n",
    "encoded = bert_tokenizer.encode_plus(df['Text'][0], is_split_into_words=False, return_tensors=\"pt\")\n",
    "encoded_dict = {**encoded}\n",
    "input_ids = encoded_dict['input_ids']\n",
    "token_type_ids = encoded_dict['token_type_ids']\n",
    "attention_mask = encoded_dict['attention_mask']\n",
    "\n",
    "n = input_ids.shape[1]\n",
    "\n",
    "passes = [(i * 256, i * 256 + 512) for i in range(int(n / 256) - 1)]\n",
    "if len(passes) == 0:\n",
    "    # Documents < 512 tokens\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**encoded)\n",
    "\n",
    "    # Get all hidden states\n",
    "    states = outputs.hidden_states\n",
    "    # Stack and sum all requested layers\n",
    "    output = torch.stack([states[i].squeeze() for i in [-4, -3, -2, -1]])\n",
    "    query_t = torch.mean(output, dim=0)\n",
    "    print(get_word_embedding(encoded, query_t, 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "626586b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = bert_tokenizer.encode_plus(df['Text'][0], is_split_into_words=False, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b934aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(i):\n",
    "    token_ids_word = np.where(np.array(encoded.word_ids()) == i)\n",
    "    word_piece = bert_tokenizer.convert_ids_to_tokens(encoded['input_ids'].squeeze()[token_ids_word])\n",
    "    word_piece = [w.split(\"##\")[1] if w.startswith(\"##\") else w for w in word_piece]\n",
    "    print(''.join(word_piece))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a24c560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    "print(df['Text'][0][274])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef8e4ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277\n"
     ]
    },
    {
     "data": {
      "text/plain": "'her'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string = \"This is a test string.\"\n",
    "indices = [i for i, x in enumerate(df['Text'][0]) if x == \" \"]\n",
    "print([i for i  in indices if i > 274][0])\n",
    "df['Text'][0][274:277]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09465a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "her\n"
     ]
    }
   ],
   "source": [
    "def get_token_index_from_char_offset(char_offset, text):\n",
    "    indices = [i for i, x in enumerate(text) if x == \" \"]\n",
    "    end_idx = [i for i  in indices if i > char_offset][0]\n",
    "    return text[char_offset:end_idx]\n",
    "    \n",
    "print(get_token_index_from_char_offset(274, df['Text'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a922b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zoe\n",
      "Telford\n",
      "-\n",
      "-\n",
      "played\n",
      "the\n",
      "police\n",
      "officer\n",
      "girlfriend\n",
      "of\n",
      "Simon\n",
      ",\n",
      "Maggie\n",
      ".\n",
      "Dumped\n",
      "by\n",
      "Simon\n",
      "in\n",
      "the\n",
      "final\n",
      "episode\n",
      "of\n",
      "series\n",
      "1\n",
      ",\n",
      "after\n",
      "he\n",
      "slept\n",
      "with\n",
      "Jenny\n",
      ",\n",
      "and\n",
      "is\n",
      "not\n",
      "seen\n",
      "again\n",
      ".\n",
      "Phoebe\n",
      "Thomas\n",
      "played\n",
      "Cheryl\n",
      "Cassidy\n",
      ",\n",
      "Pauline\n",
      "'\n",
      "s\n",
      "friend\n",
      "and\n",
      "also\n",
      "a\n",
      "year\n",
      "11\n",
      "pupil\n",
      "in\n",
      "Simon\n",
      "'\n",
      "s\n",
      "class\n",
      ".\n",
      "Dumped\n",
      "her\n",
      "boyfriend\n",
      "following\n",
      "Simon\n",
      "'\n",
      "s\n",
      "advice\n",
      "after\n",
      "he\n",
      "wouldn\n",
      "'\n",
      "t\n",
      "have\n",
      "sex\n",
      "with\n",
      "her\n",
      "but\n",
      "later\n",
      "realised\n",
      "this\n",
      "was\n",
      "due\n",
      "to\n",
      "him\n",
      "catching\n",
      "crabs\n",
      "off\n",
      "her\n",
      "friend\n",
      "Pauline\n"
     ]
    }
   ],
   "source": [
    "tokens = [get_token(i) for i in range(90)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e063cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "indices = []\n",
    "for i in range(df.shape[0]):\n",
    "    is_okay = df['Text'][i].index(df['A'][i]) == df['A-offset'][i] and df['Text'][i].index(df['B'][i]) == df['B-offset'][i] \\\n",
    "            and df['Text'][i].index(df['Pronoun'][i]) == df['Pronoun-offset'][i]\n",
    "    if is_okay:\n",
    "        indices.append(i)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "116e1c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "474"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "filtered_df = df[df.index.isin(indices)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "     index                ID  \\\n0        3     development-4   \n1        7     development-8   \n2        9    development-10   \n3       10    development-11   \n4       28    development-29   \n..     ...               ...   \n469   1987  development-1988   \n470   1993  development-1994   \n471   1994  development-1995   \n472   1997  development-1998   \n473   1998  development-1999   \n\n                                                  Text Pronoun  \\\n0    The current members of Crime have also perform...     his   \n1    Slant Magazine's Sal Cinquemani viewed the alb...     his   \n2    Shaftesbury's UK partners in the production of...     she   \n3    William Shatner portraying writer Mark Twain; ...     his   \n4    Meet Mike, the shortest bully to appear on the...     him   \n..                                                 ...     ...   \n469  On the morning of her disappearance, Alison st...     her   \n470  She then determined on becoming an artist, and...     her   \n471  Bill Grainer Bill Grainer is a Grammy certifie...     her   \n472  Grant played the part in Trevor Nunn's movie a...     she   \n473  The fashion house specialised in hand-printed ...     She   \n\n     Pronoun-offset                   A  A-offset  A-coref                 B  \\\n0               321                Hell       174    False   Henry Rosenthal   \n1               337            Greg Kot       173    False  Robert Christgau   \n2               329  Christina Jennings       196     True  Kirstine Stewart   \n3               300    Peter Mansbridge       168    False       David Onley   \n4               250       Mayhem Miller       153    False     Eddie Alvarez   \n..              ...                 ...       ...      ...               ...   \n469              18              Alison        37     True             Jenna   \n470             407      Louise Breslau       363     True      Bashkirtseff   \n471             212          Linda Eder       131    False   Jennifer Hudson   \n472             348               Maria       259     True   Imelda Staunton   \n473             284               Helen       145     True   Suzanne Bartsch   \n\n     B-offset  B-coref                                                URL  \n0         336     True          http://en.wikipedia.org/wiki/Crime_(band)  \n1         377     True  http://en.wikipedia.org/wiki/The_Truth_About_L...  \n2         226    False     http://en.wikipedia.org/wiki/Murdoch_Mysteries  \n3         212     True     http://en.wikipedia.org/wiki/Murdoch_Mysteries  \n4         189    False  http://en.wikipedia.org/wiki/List_of_Bully_Bea...  \n..        ...      ...                                                ...  \n469       101    False    http://en.wikipedia.org/wiki/Alison_DiLaurentis  \n470       384    False    http://en.wikipedia.org/wiki/Marie_Bashkirtseff  \n471       147     True          http://en.wikipedia.org/wiki/Bill_Grainer  \n472       266    False  http://en.wikipedia.org/wiki/Sir_Andrew_Aguecheek  \n473       208    False           http://en.wikipedia.org/wiki/Helen_David  \n\n[474 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>ID</th>\n      <th>Text</th>\n      <th>Pronoun</th>\n      <th>Pronoun-offset</th>\n      <th>A</th>\n      <th>A-offset</th>\n      <th>A-coref</th>\n      <th>B</th>\n      <th>B-offset</th>\n      <th>B-coref</th>\n      <th>URL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>development-4</td>\n      <td>The current members of Crime have also perform...</td>\n      <td>his</td>\n      <td>321</td>\n      <td>Hell</td>\n      <td>174</td>\n      <td>False</td>\n      <td>Henry Rosenthal</td>\n      <td>336</td>\n      <td>True</td>\n      <td>http://en.wikipedia.org/wiki/Crime_(band)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>development-8</td>\n      <td>Slant Magazine's Sal Cinquemani viewed the alb...</td>\n      <td>his</td>\n      <td>337</td>\n      <td>Greg Kot</td>\n      <td>173</td>\n      <td>False</td>\n      <td>Robert Christgau</td>\n      <td>377</td>\n      <td>True</td>\n      <td>http://en.wikipedia.org/wiki/The_Truth_About_L...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9</td>\n      <td>development-10</td>\n      <td>Shaftesbury's UK partners in the production of...</td>\n      <td>she</td>\n      <td>329</td>\n      <td>Christina Jennings</td>\n      <td>196</td>\n      <td>True</td>\n      <td>Kirstine Stewart</td>\n      <td>226</td>\n      <td>False</td>\n      <td>http://en.wikipedia.org/wiki/Murdoch_Mysteries</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10</td>\n      <td>development-11</td>\n      <td>William Shatner portraying writer Mark Twain; ...</td>\n      <td>his</td>\n      <td>300</td>\n      <td>Peter Mansbridge</td>\n      <td>168</td>\n      <td>False</td>\n      <td>David Onley</td>\n      <td>212</td>\n      <td>True</td>\n      <td>http://en.wikipedia.org/wiki/Murdoch_Mysteries</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28</td>\n      <td>development-29</td>\n      <td>Meet Mike, the shortest bully to appear on the...</td>\n      <td>him</td>\n      <td>250</td>\n      <td>Mayhem Miller</td>\n      <td>153</td>\n      <td>False</td>\n      <td>Eddie Alvarez</td>\n      <td>189</td>\n      <td>False</td>\n      <td>http://en.wikipedia.org/wiki/List_of_Bully_Bea...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>469</th>\n      <td>1987</td>\n      <td>development-1988</td>\n      <td>On the morning of her disappearance, Alison st...</td>\n      <td>her</td>\n      <td>18</td>\n      <td>Alison</td>\n      <td>37</td>\n      <td>True</td>\n      <td>Jenna</td>\n      <td>101</td>\n      <td>False</td>\n      <td>http://en.wikipedia.org/wiki/Alison_DiLaurentis</td>\n    </tr>\n    <tr>\n      <th>470</th>\n      <td>1993</td>\n      <td>development-1994</td>\n      <td>She then determined on becoming an artist, and...</td>\n      <td>her</td>\n      <td>407</td>\n      <td>Louise Breslau</td>\n      <td>363</td>\n      <td>True</td>\n      <td>Bashkirtseff</td>\n      <td>384</td>\n      <td>False</td>\n      <td>http://en.wikipedia.org/wiki/Marie_Bashkirtseff</td>\n    </tr>\n    <tr>\n      <th>471</th>\n      <td>1994</td>\n      <td>development-1995</td>\n      <td>Bill Grainer Bill Grainer is a Grammy certifie...</td>\n      <td>her</td>\n      <td>212</td>\n      <td>Linda Eder</td>\n      <td>131</td>\n      <td>False</td>\n      <td>Jennifer Hudson</td>\n      <td>147</td>\n      <td>True</td>\n      <td>http://en.wikipedia.org/wiki/Bill_Grainer</td>\n    </tr>\n    <tr>\n      <th>472</th>\n      <td>1997</td>\n      <td>development-1998</td>\n      <td>Grant played the part in Trevor Nunn's movie a...</td>\n      <td>she</td>\n      <td>348</td>\n      <td>Maria</td>\n      <td>259</td>\n      <td>True</td>\n      <td>Imelda Staunton</td>\n      <td>266</td>\n      <td>False</td>\n      <td>http://en.wikipedia.org/wiki/Sir_Andrew_Aguecheek</td>\n    </tr>\n    <tr>\n      <th>473</th>\n      <td>1998</td>\n      <td>development-1999</td>\n      <td>The fashion house specialised in hand-printed ...</td>\n      <td>She</td>\n      <td>284</td>\n      <td>Helen</td>\n      <td>145</td>\n      <td>True</td>\n      <td>Suzanne Bartsch</td>\n      <td>208</td>\n      <td>False</td>\n      <td>http://en.wikipedia.org/wiki/Helen_David</td>\n    </tr>\n  </tbody>\n</table>\n<p>474 rows × 12 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'RangeIndex' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mfiltered_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mText\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfiltered_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mPronoun\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: 'RangeIndex' object is not callable"
     ]
    }
   ],
   "source": [
    "filtered_df['Text'].index(filtered_df['Pronoun'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idxs = []\n",
    "for i in range(filtered_df.shape[0]):\n",
    "    idx = filtered_df['Text'][i].index(filtered_df['Pronoun'][i])\n",
    "    idxs.append(idx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idxs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_word_embedding(encoded, output, idx):\n",
    "    token_ids_word = np.where(np.array(encoded.word_ids()) == idx)\n",
    "    print(token_ids_word)\n",
    "    word_tokens_output = output[token_ids_word]\n",
    "\n",
    "    return word_tokens_output.mean(dim=0)\n",
    "\n",
    "encoded = bert_tokenizer.encode_plus(filtered_df['Text'][0], is_split_into_words=False, return_tensors=\"pt\")\n",
    "encoded_dict = {**encoded}\n",
    "input_ids = encoded_dict['input_ids']\n",
    "token_type_ids = encoded_dict['token_type_ids']\n",
    "attention_mask = encoded_dict['attention_mask']\n",
    "\n",
    "n = input_ids.shape[1]\n",
    "\n",
    "passes = [(i * 256, i * 256 + 512) for i in range(int(n / 256) - 1)]\n",
    "if len(passes) == 0:\n",
    "    # Documents < 512 tokens\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**encoded)\n",
    "\n",
    "    # Get all hidden states\n",
    "    states = outputs.hidden_states\n",
    "    # Stack and sum all requested layers\n",
    "    output = torch.stack([states[i].squeeze() for i in [-4, -3, -2, -1]])\n",
    "    query_t = torch.mean(output, dim=0)\n",
    "    print(get_word_embedding(encoded, query_t, 90))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoded = bert_tokenizer.encode_plus(filtered_df['Text'][0], is_split_into_words=False, return_tensors=\"pt\")\n",
    "encoded_dict = {**encoded}\n",
    "input_ids = encoded_dict['input_ids']\n",
    "token_type_ids = encoded_dict['token_type_ids']\n",
    "attention_mask = encoded_dict['attention_mask']\n",
    "\n",
    "bert_tokenizer.convert_ids_to_tokens(input_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokens = bert_tokenizer.convert_ids_to_tokens(input_ids.squeeze())\n",
    "tokens.index(filtered_df['Pronoun'][0])\n",
    "tokens.index(filtered_df['A'][0])\n",
    "tokens.index(filtered_df['B'][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count = 0\n",
    "indices = []\n",
    "for i in range(df.shape[0]):\n",
    "    is_okay = df['Text'][i].index(df['A'][i]) == df['A-offset'][i] and df['Text'][i].index(df['B'][i]) == df['B-offset'][i] \\\n",
    "            and df['Text'][i].index(df['Pronoun'][i]) == df['Pronoun-offset'][i]\n",
    "    if is_okay:\n",
    "        indices.append(i)\n",
    "        count += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "spacy_tokenizer = nlp.tokenizer\n",
    "\n",
    "tokens = spacy_tokenizer(df['Text'][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[token.text for token in tokens]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len([token for token in tokens])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(df['Text'][0].split(\" \"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bert_tokenizer.encode_plus([token.text for token in tokens], return_tensors='pt', is_split_into_words=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def bs(list_, target_):\n",
    "    lo, hi = 0, len(list_) -1\n",
    "\n",
    "    while lo < hi:\n",
    "        mid = lo + int((hi - lo) / 2)\n",
    "\n",
    "        if target_ < list_[mid]:\n",
    "            hi = mid\n",
    "        elif target_ > list_[mid]:\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            return mid + 1\n",
    "    return lo\n",
    "\n",
    "def bs_(list_, target_):\n",
    "    lo, hi = 0, len(list_) -1\n",
    "\n",
    "    while lo < hi:\n",
    "        mid = lo + int((hi - lo) / 2)\n",
    "\n",
    "        if target_ < list_[mid]:\n",
    "            hi = mid\n",
    "        elif target_ > list_[mid]:\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            return mid\n",
    "    return lo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "doc = nlp(df['Text'][0])\n",
    "\n",
    "\n",
    "# char offset to token offset\n",
    "lens = [token.idx for token in doc]\n",
    "mention_offset1 = bs(lens, 274) - 1\n",
    "# mention_offset2 = bs(lens, char_offset2) - 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mention_offset1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "doc[57]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "pronoun_token_offsets, a_token_offsets, b_token_offsets, text_tokens = [], [], [], []\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    pronoun_offset = df['Pronoun-offset'][i]\n",
    "    a_offset = df['A-offset'][i]\n",
    "    b_offset = df['B-offset'][i]\n",
    "    doc = nlp(df['Text'][i])\n",
    "    lens = [token.idx for token in doc]\n",
    "    pronoun_token_offsets.append(bs(lens, pronoun_offset) - 1)\n",
    "    a_token_offsets.append(bs(lens, a_offset) - 1)\n",
    "    b_token_offsets.append(bs(lens, b_offset) - 1)\n",
    "    text_tokens.append([token.text for token in doc])\n",
    "\n",
    "token_df = pd.DataFrame(list(zip(pronoun_token_offsets, a_token_offsets, b_token_offsets, text_tokens)),\n",
    "                  columns=[\"Pronoun-token-offset\", \"A-token-offset\", \"B-token-offset\", \"Text Tokens\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "      Pronoun-token-offset  A-token-offset  B-token-offset  \\\n0                       57              39              42   \n1                       56              46              49   \n2                       54              35              49   \n3                       66              35              70   \n4                       85              42              57   \n...                    ...             ...             ...   \n1995                    84              53              63   \n1996                    50              22              45   \n1997                    72              56              58   \n1998                    56              27              42   \n1999                    78              63              74   \n\n                                            Text Tokens  \n0     [Zoe, Telford, --, played, the, police, office...  \n1     [He, grew, up, in, Evanston, ,, Illinois, the,...  \n2     [He, had, been, reelected, to, Congress, ,, bu...  \n3     [The, current, members, of, Crime, have, also,...  \n4     [Her, Santa, Fe, Opera, debut, in, 2005, was, ...  \n...                                                 ...  \n1995  [Faye, 's, third, husband, ,, Paul, Resnick, ,...  \n1996  [The, plot, of, the, film, focuses, on, the, l...  \n1997  [Grant, played, the, part, in, Trevor, Nunn, '...  \n1998  [The, fashion, house, specialised, in, hand, -...  \n1999  [Watkins, was, a, close, friend, of, Hess, ', ...  \n\n[2000 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pronoun-token-offset</th>\n      <th>A-token-offset</th>\n      <th>B-token-offset</th>\n      <th>Text Tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>57</td>\n      <td>39</td>\n      <td>42</td>\n      <td>[Zoe, Telford, --, played, the, police, office...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>56</td>\n      <td>46</td>\n      <td>49</td>\n      <td>[He, grew, up, in, Evanston, ,, Illinois, the,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>54</td>\n      <td>35</td>\n      <td>49</td>\n      <td>[He, had, been, reelected, to, Congress, ,, bu...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>66</td>\n      <td>35</td>\n      <td>70</td>\n      <td>[The, current, members, of, Crime, have, also,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>85</td>\n      <td>42</td>\n      <td>57</td>\n      <td>[Her, Santa, Fe, Opera, debut, in, 2005, was, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>84</td>\n      <td>53</td>\n      <td>63</td>\n      <td>[Faye, 's, third, husband, ,, Paul, Resnick, ,...</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>50</td>\n      <td>22</td>\n      <td>45</td>\n      <td>[The, plot, of, the, film, focuses, on, the, l...</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>72</td>\n      <td>56</td>\n      <td>58</td>\n      <td>[Grant, played, the, part, in, Trevor, Nunn, '...</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>56</td>\n      <td>27</td>\n      <td>42</td>\n      <td>[The, fashion, house, specialised, in, hand, -...</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>78</td>\n      <td>63</td>\n      <td>74</td>\n      <td>[Watkins, was, a, close, friend, of, Hess, ', ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "df = pd.concat([df,token_df],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['Text'][0][df['A-token-offset'][0]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = nlp.tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len([token for token in tokenizer(df['A'][0])])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[token.text for token in doc][df['A-token-offset'][0]:df['A-token-offset'][0]+len([token for token in tokenizer(df['A'][0])])]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, tokens, corefs, speakers, sentences, filename):\n",
    "        self.tokens = tokens\n",
    "        self.corefs = corefs\n",
    "        self.speakers = speakers\n",
    "        self.sentences = sentences\n",
    "        self.filename = filename\n",
    "\n",
    "        self.tags = None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GapDocument:\n",
    "    def __init__(self, tokens, corefs, filename):\n",
    "        self.tokens = tokens\n",
    "        self.corefs = corefs\n",
    "        self.filename = filename\n",
    "\n",
    "        self.tags = None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.tokens[idx], self.corefs[idx])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'Document containing {len(self.tokens)} tokens'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    @cached_property\n",
    "    def sents(self):\n",
    "        \"\"\" Regroup raw_text into sentences \"\"\"\n",
    "\n",
    "        # Get sentence boundaries\n",
    "        sent_idx = [idx+1\n",
    "                    for idx, token in enumerate(self.tokens)\n",
    "                    if token in ['.', '?', '!']]\n",
    "\n",
    "        # Regroup (returns list of lists)\n",
    "        return [self.tokens[i1:i2] for i1, i2 in bert_pairwise([0] + sent_idx)]\n",
    "\n",
    "    def spans(self):\n",
    "        return [Span(i1=i[0], i2=i[-1], id=idx,\n",
    "                     speaker=None)\n",
    "                for idx, i in enumerate(compute_idx_spans(self.sents))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_gap_file(filename):\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        # Tokenize GAP example\n",
    "        doc = nlp(df['Text'][i])\n",
    "\n",
    "        # Convert char offset to token offset\n",
    "        lens = [token.idx for token in doc]\n",
    "        pronoun_offset = bs(lens, df['Pronoun-offset'][i]) - 1\n",
    "        a_offset = bs(lens, df['A-offset'][i]) - 1\n",
    "        b_offset = bs(lens, df['B-offset'][i]) - 1\n",
    "\n",
    "        doc = GapDocument([token.text for token in doc], {'pronoun': pronoun_offset, }, filename)\n",
    "        documents.append(doc)\n",
    "\n",
    "    #\n",
    "    # token_df = pd.DataFrame(list(zip(pronoun_token_offsets, a_token_offsets, b_token_offsets, text_tokens)),\n",
    "    #                         columns=[\"Pronoun-token-offset\", \"A-token-offset\", \"B-token-offset\", \"Text Tokens\"])\n",
    "    #\n",
    "    # df = pd.concat([df, token_df], axis=1)\n",
    "\n",
    "    return documents\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}